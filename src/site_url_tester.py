#!/usr/bin/env python3
import json
import requests
import os
import warnings
import re
from urllib3.exceptions import InsecureRequestWarning
from pathlib import Path

warnings.simplefilter('ignore', InsecureRequestWarning)

class SiteURLTester:
    def __init__(self, config_file: str = None):
        """åˆå§‹åŒ–ç«™ç‚¹URLæµ‹è¯•å™¨"""
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆè„šæœ¬æ–‡ä»¶çš„ä¸Šä¸¤çº§ç›®å½•ï¼‰
        self.base_dir = Path(__file__).parent.parent.absolute()

        # è®¾ç½®é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        if config_file is None:
            config_file = self.base_dir / "config" / "url_tester_config.json"
        else:
            config_file = Path(config_file)
            if not config_file.is_absolute():
                config_file = self.base_dir / config_file

        self.config_file = str(config_file)
        self.config = self.load_config()
        self.last_site = None

    def safe_print(self, text: str):
        """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
        try:
            print(text)
        except UnicodeEncodeError:
            # Windowsç¯å¢ƒä¸‹å¤„ç†emojiç¼–ç é—®é¢˜
            safe_text = text.encode('ascii', 'ignore').decode('ascii')
            print(safe_text)
        
    def load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            # TVBoxç›¸å…³é…ç½®
            "tvbox_json_dir": str(self.base_dir / "files" / "TVBoxOSC" / "tvbox" / "json"),
            "tvbox_files_config": {
                "wogg.json": "ç©å¶",
                "mogg.json": "æœ¨å¶",
                "lb.json": "èœ¡ç¬”",
                "zz.json": "è‡³è‡»",
                "yyds.json": "å¤šå¤š",
                "og.json": "æ¬§å“¥",
                "ex.json": "äºŒå°",
                "hb.json": "è™æ–‘",
                "sd.json": "é—ªç”µ"
            },
            
            # URLæµ‹è¯•é…ç½®
            "search_paths": {
                'é—ªç”µ': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'æ¬§å“¥': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'å¤šå¤š': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'èœ¡ç¬”': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'è‡³è‡»': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'è™æ–‘': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'ç©å¶': '/vodsearch/-------------.html?wd=ä»™å°æœ‰æ ‘',
                'æœ¨å¶': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘',
                'äºŒå°': '/index.php/vod/search.html?wd=ä»™å°æœ‰æ ‘'
            },
            
            "keyword_validation": {
                'é—ªç”µ': 'class="search-stat"',
                'æ¬§å“¥': 'class="search-stat"',
                'å¤šå¤š': 'class="search-stat"',
                'èœ¡ç¬”': 'class="search-stat"',
                'è‡³è‡»': 'class="search-stat"',
                'è™æ–‘': 'class="search-stat"',
                'ç©å¶': 'class="search-stat"',
                'æœ¨å¶': 'class="search-stat"',
                'äºŒå°': 'class="search-stat"'
            },
            
            "url_weights": {
                "æœ¨å¶": {"https://aliii.deno.dev": 60, "http://149.88.87.72:5666": 60},
                "è‡³è‡»": {"http://www.xhww.net": 10, "http://xhww.net": 10}
            },
            
            # ä»£ç†é…ç½®
            "proxy": {
                "enabled": False,
                "proxies": {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"}
            },
            
            # æµ‹è¯•é…ç½®
            "test_timeout": 7,
            "default_weight": 50
        }
        
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # å°†é…ç½®æ–‡ä»¶ä¸­çš„ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºåŸºäºé¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
                    for key, value in user_config.items():
                        if key.endswith('_path') or key.endswith('_dir') or key.endswith('_file'):
                            if isinstance(value, str) and not os.path.isabs(value):
                                user_config[key] = str(self.base_dir / value)
                    default_config.update(user_config)
            else:
                with open(self.config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, ensure_ascii=False, indent=4)
        except Exception as e:
            self.safe_print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        return default_config
    
    def log_message(self, message, site_name=None, step=""):
        """æ ¼å¼åŒ–æ‰“å°æ—¥å¿—æ¶ˆæ¯"""
        status_emojis = {
            '[å¼€å§‹]': 'ğŸš€', '[æˆåŠŸ]': 'âœ…', '[å®Œæˆ]': 'ğŸ‰', '[å¤±è´¥]': 'âŒ',
            '[è¶…æ—¶]': 'â³', '[è­¦å‘Š]': 'âš ï¸', '[é”™è¯¯]': 'ğŸš¨', '[ä¿¡æ¯]': 'â„¹ï¸',
            '[é€‰æ‹©]': 'ğŸ”', '[è¿æ¥å¤±è´¥]': 'ğŸ”Œ'
        }

        if site_name and site_name != self.last_site:
            self.safe_print(f"\n{'âœ¨ ' + '='*38 + ' âœ¨'}\nğŸŒ [ç«™ç‚¹: {site_name}]\n{'âœ¨ ' + '='*38 + ' âœ¨'}")
            self.last_site = site_name

        for status, emoji in status_emojis.items():
            if status in message:
                message = message.replace(status, f"{status} {emoji}")
                break
        else:
            message += " ğŸ“¢"

        self.safe_print(f"[{step}] {message}" if step else message)
    
    def extract_urls_from_tvbox(self) -> dict:
        """ä»TVBox JSONæ–‡ä»¶æˆ–data/test.jsonä¸­æå–URLä¿¡æ¯"""
        self.log_message("[å¼€å§‹] æå–URLä¿¡æ¯...", step="æå–URL")

        extracted_urls = {}

        # é¦–å…ˆå°è¯•ä»data/test.jsonè¯»å–ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        test_json_path = self.base_dir / "data" / "test.json"
        if test_json_path.exists():
            self.log_message("[ä¿¡æ¯] å‘ç°data/test.jsonï¼Œä½¿ç”¨æ­¤æ–‡ä»¶ä½œä¸ºæ•°æ®æº", step="æå–URL")
            try:
                with open(test_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # data/test.jsonæ ¼å¼: {"ç«™ç‚¹å": ["url1", "url2", ...]}
                for site_name, urls in data.items():
                    if isinstance(urls, list) and urls:
                        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’Œæ— æ•ˆURL
                        valid_urls = [url.strip() for url in urls if url and url.strip()]
                        if valid_urls:
                            extracted_urls[site_name] = valid_urls
                            self.log_message(f"[æˆåŠŸ] {site_name}: æ‰¾åˆ° {len(valid_urls)} ä¸ªURL",
                                           site_name, "æå–URL")
                        else:
                            self.log_message(f"[è­¦å‘Š] {site_name} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL", site_name, "æå–URL")
                    else:
                        self.log_message(f"[è­¦å‘Š] {site_name} çš„URLåˆ—è¡¨æ ¼å¼æ— æ•ˆ", site_name, "æå–URL")

                self.log_message(f"[å®Œæˆ] ä»data/test.jsonå…±æå–åˆ° {len(extracted_urls)} ä¸ªç«™ç‚¹çš„URLä¿¡æ¯", step="æå–URL")
                return extracted_urls

            except Exception as e:
                self.log_message(f"[é”™è¯¯] è¯»å–data/test.jsonå¤±è´¥: {e}", step="æå–URL")
                # ç»§ç»­å°è¯•TVBoxç›®å½•

        # å¦‚æœdata/test.jsonä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå°è¯•ä»TVBoxç›®å½•è¯»å–
        tvbox_dir = self.config['tvbox_json_dir']

        if not os.path.exists(tvbox_dir):
            self.log_message(f"[é”™è¯¯] TVBoxç›®å½•ä¸å­˜åœ¨: {tvbox_dir}ï¼Œä¸”data/test.jsonä¹Ÿä¸å¯ç”¨", step="æå–URL")
            return {}

        self.log_message(f"[ä¿¡æ¯] ä»TVBoxç›®å½•è¯»å–: {tvbox_dir}", step="æå–URL")

        for filename, site_name in self.config['tvbox_files_config'].items():
            file_path = os.path.join(tvbox_dir, filename)

            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # æå–Domainså­—æ®µä¸­çš„URL
                    if 'Domains' in data and isinstance(data['Domains'], list):
                        urls = []
                        for domain_info in data['Domains']:
                            if isinstance(domain_info, dict) and 'url' in domain_info:
                                urls.append(domain_info['url'])
                            elif isinstance(domain_info, str):
                                urls.append(domain_info)

                        if urls:
                            extracted_urls[site_name] = urls
                            self.log_message(f"[æˆåŠŸ] {filename} -> {site_name}: æ‰¾åˆ° {len(urls)} ä¸ªURL",
                                           site_name, "æå–URL")
                        else:
                            self.log_message(f"[è­¦å‘Š] {filename} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL", site_name, "æå–URL")
                    else:
                        self.log_message(f"[è­¦å‘Š] {filename} ä¸­æœªæ‰¾åˆ°Domainså­—æ®µ", site_name, "æå–URL")
                else:
                    self.log_message(f"[è­¦å‘Š] æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", step="æå–URL")

            except Exception as e:
                self.log_message(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}", step="æå–URL")
                continue

        self.log_message(f"[å®Œæˆ] å…±æå–åˆ° {len(extracted_urls)} ä¸ªç«™ç‚¹çš„URLä¿¡æ¯", step="æå–URL")
        return extracted_urls

    def test_url_availability(self, url, site_name=None):
        """æµ‹è¯•å•ä¸ªURLçš„å¯ç”¨æ€§"""
        search_path = self.config['search_paths'].get(site_name)
        test_url_str = f"{url.strip()}{search_path}" if search_path else url.strip()
        keyword = self.config['keyword_validation'].get(site_name)

        try:
            response = requests.get(test_url_str, timeout=self.config['test_timeout'], verify=False)
            latency = response.elapsed.total_seconds()

            if response.status_code == 200:
                has_keyword = keyword in response.text if keyword else True
                if has_keyword:
                    self.log_message(f"[æˆåŠŸ] URL {test_url_str} å»¶è¿Ÿ: {latency:.2f}s{'ï¼ŒåŒ…å«å…³é”®å­— ' + keyword if keyword else ''}",
                                    site_name, "æµ‹è¯•URL")
                else:
                    self.log_message(f"[æˆåŠŸ] URL {test_url_str} å»¶è¿Ÿ: {latency:.2f}sï¼Œä½†ä¸åŒ…å«å…³é”®å­— '{keyword}'",
                                    site_name, "æµ‹è¯•URL")
                    self.log_message(f"[è­¦å‘Š] è¯¥URLè¿”å›200ä½†æ— å…³é”®å­—ï¼Œå¯èƒ½å­˜åœ¨Cloudflareç›¾æˆ–å…¶ä»–åçˆ¬æœºåˆ¶",
                                    site_name, "æµ‹è¯•URL")
                return latency, has_keyword
            else:
                self.log_message(f"[å¤±è´¥] URL {test_url_str} è¿”å›HTTPé”™è¯¯: çŠ¶æ€ç  {response.status_code}",
                                site_name, "æµ‹è¯•URL")
                return None, None

        except requests.RequestException as e:
            error_type = "[è¶…æ—¶]" if isinstance(e, requests.Timeout) else "[è¿æ¥å¤±è´¥]" if isinstance(e, requests.ConnectionError) else "[é”™è¯¯]"
            self.log_message(f"{error_type} URL {test_url_str} {str(e)}", site_name, "æµ‹è¯•URL")

            # å°è¯•ä»£ç†è®¿é—®
            if self.config['proxy'].get("enabled", False):
                try:
                    response = requests.get(test_url_str, timeout=self.config['test_timeout'],
                                          verify=False, proxies=self.config['proxy']['proxies'])
                    latency = response.elapsed.total_seconds()

                    if response.status_code == 200:
                        has_keyword = keyword in response.text if keyword else True
                        if has_keyword:
                            self.log_message(f"[æˆåŠŸ] URL {test_url_str} é€šè¿‡ä»£ç†è®¿é—®æˆåŠŸï¼Œå»¶è¿Ÿ: {latency:.2f}s{'ï¼ŒåŒ…å«å…³é”®å­— ' + keyword if keyword else ''}",
                                            site_name, "æµ‹è¯•URL")
                        else:
                            self.log_message(f"[æˆåŠŸ] URL {test_url_str} é€šè¿‡ä»£ç†è®¿é—®æˆåŠŸï¼Œå»¶è¿Ÿ: {latency:.2f}sï¼Œä½†ä¸åŒ…å«å…³é”®å­— '{keyword}'",
                                            site_name, "æµ‹è¯•URL")
                        self.log_message(f"[ä¿¡æ¯] éä»£ç†è®¿é—®å¤±è´¥ä½†ä»£ç†è®¿é—®æˆåŠŸï¼Œå»ºè®®é…ç½®ä»£ç†",
                                        site_name, "æµ‹è¯•URL")
                        return latency, has_keyword
                    else:
                        self.log_message(f"[å¤±è´¥] URL {test_url_str} é€šè¿‡ä»£ç†è®¿é—®è¿”å›HTTPé”™è¯¯: çŠ¶æ€ç  {response.status_code}",
                                        site_name, "æµ‹è¯•URL")
                        return None, None

                except requests.RequestException as proxy_e:
                    proxy_error_type = "[è¶…æ—¶]" if isinstance(proxy_e, requests.Timeout) else "[è¿æ¥å¤±è´¥]" if isinstance(proxy_e, requests.ConnectionError) else "[é”™è¯¯]"
                    self.log_message(f"{proxy_error_type} URL {test_url_str} ä»£ç†è®¿é—®ä¹Ÿå¤±è´¥: {str(proxy_e)}",
                                    site_name, "æµ‹è¯•URL")

            return None, None

    def select_best_url(self, urls, site_name=None):
        """é€‰æ‹©æœ€ä½³URL"""
        if not isinstance(urls, list):
            return urls

        if len(urls) == 1:
            self.log_message(f"[ä¿¡æ¯] ç«™ç‚¹ä»…æœ‰å•ä¸€URL {urls[0]}ï¼Œç›´æ¥ä½¿ç”¨", site_name, "é€‰æ‹©æœ€ä½³URL")
            return urls[0]

        weights = self.config['url_weights'].get(site_name, {})
        default_weight = self.config['default_weight']
        url_data = [(url, weights.get(url, default_weight)) for url in urls]
        sorted_urls = sorted(url_data, key=lambda x: -x[1])

        url_results = {}
        highest_weight = sorted_urls[0][1] if sorted_urls else 0
        has_keyword_at_highest_weight = False

        for url, weight in sorted_urls:
            if has_keyword_at_highest_weight and weight < highest_weight:
                self.log_message(f"[ä¿¡æ¯] æœ€é«˜æƒé‡ {highest_weight} å·²æ‰¾åˆ°åŒ…å«å…³é”®å­—çš„URLï¼Œè·³è¿‡ä½æƒé‡URL {url} (æƒé‡: {weight}) æµ‹è¯•",
                                site_name, "é€‰æ‹©æœ€ä½³URL")
                continue

            latency, has_keyword = self.test_url_availability(url, site_name)
            if latency is not None:
                url_results[url] = (latency, has_keyword, weight)
                if has_keyword and weight == highest_weight:
                    has_keyword_at_highest_weight = True

        if not url_results:
            self.log_message(f"[è­¦å‘Š] å½“å‰æ— å¯ç”¨URL", site_name, "é€‰æ‹©æœ€ä½³URL")
            return None

        # æ’åºï¼šå…³é”®å­— > æƒé‡ > å»¶è¿Ÿ
        sorted_results = sorted(url_results.items(), key=lambda x: (-x[1][1], -x[1][2], x[1][0]))

        best_url, (best_latency, best_has_keyword, best_weight) = sorted_results[0]
        self.log_message(f"[é€‰æ‹©] æœ€ä¼˜URL: {best_url} (å…³é”®å­—: {best_has_keyword}, æƒé‡: {best_weight}, å»¶è¿Ÿ: {best_latency:.2f}s)",
                        site_name, "é€‰æ‹©æœ€ä½³URL")

        return best_url, url_results

    def save_results_to_json(self, results, output_file=None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶è·¯å¾„
            if output_file is None:
                output_file = str(self.base_dir / "data" / "test_results.json")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            # å‡†å¤‡JSONæ•°æ®
            json_data = {
                "timestamp": __import__('datetime').datetime.now().isoformat(),
                "summary": {
                    "total_sites": len(results),
                    "success_sites": len([r for r in results.values() if r['best_url']]),
                    "failed_sites": len([r for r in results.values() if not r['best_url']])
                },
                "sites": {}
            }

            for site_name, result in results.items():
                site_data = {
                    "site_name": site_name,
                    "best_url": result['best_url'],
                    "status": "success" if result['best_url'] else "failed",
                    "urls": []
                }

                if 'url_results' in result and result['url_results']:
                    for url, (latency, has_keyword, weight) in result['url_results'].items():
                        url_data = {
                            "url": url,
                            "latency": round(latency, 2),
                            "has_keyword": has_keyword,
                            "weight": weight,
                            "is_best": url == result['best_url']
                        }
                        site_data['urls'].append(url_data)

                    # æŒ‰æ˜¯å¦ä¸ºæœ€ä½³URLæ’åºï¼Œæœ€ä½³çš„åœ¨å‰é¢
                    site_data['urls'].sort(key=lambda x: (not x['is_best'], x['latency']))

                json_data['sites'][site_name] = site_data

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            self.log_message(f"[æˆåŠŸ] æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° {output_file}", step="ä¿å­˜ç»“æœ")
            return True

        except Exception as e:
            self.log_message(f"[é”™è¯¯] ä¿å­˜JSONç»“æœå¤±è´¥: {e}", step="ä¿å­˜ç»“æœ")
            return False

    def print_test_results(self, results):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        self.log_message("[å¼€å§‹] ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...", step="ç»“æœæŠ¥å‘Š")

        total_sites = len(results)
        success_sites = []
        failed_sites = []

        self.safe_print(f"\n{'ğŸ¯ ' + '='*50 + ' ğŸ¯'}")
        self.safe_print(f"ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        self.safe_print(f"{'ğŸ¯ ' + '='*50 + ' ğŸ¯'}")

        for site_name, result in results.items():
            if result['best_url']:
                success_sites.append(site_name)
                self.safe_print(f"\nâœ… {site_name}:")
                self.safe_print(f"   ğŸ”— æœ€ä¼˜URL: {result['best_url']}")
                if 'url_results' in result and result['url_results']:
                    self.safe_print(f"   ğŸ“ˆ æµ‹è¯•è¯¦æƒ…:")
                    for url, (latency, has_keyword, weight) in result['url_results'].items():
                        status = "âœ…" if url == result['best_url'] else "âšª"
                        keyword_status = "ğŸ”‘" if has_keyword else "âŒ"
                        self.safe_print(f"      {status} {url}")
                        self.safe_print(f"         å»¶è¿Ÿ: {latency:.2f}s | æƒé‡: {weight} | å…³é”®å­—: {keyword_status}")
            else:
                failed_sites.append(site_name)
                self.safe_print(f"\nâŒ {site_name}: æ— å¯ç”¨URL")

        self.safe_print(f"\n{'ğŸ“ˆ ' + '='*50 + ' ğŸ“ˆ'}")
        self.safe_print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        self.safe_print(f"   ğŸ¯ æ€»ç«™ç‚¹æ•°: {total_sites}")
        self.safe_print(f"   âœ… æˆåŠŸç«™ç‚¹: {len(success_sites)} ({len(success_sites)/total_sites*100:.1f}%)")
        self.safe_print(f"   âŒ å¤±è´¥ç«™ç‚¹: {len(failed_sites)} ({len(failed_sites)/total_sites*100:.1f}%)")

        if success_sites:
            self.safe_print(f"   ğŸ‰ æˆåŠŸç«™ç‚¹åˆ—è¡¨: {', '.join(success_sites)}")
        if failed_sites:
            self.safe_print(f"   âš ï¸  å¤±è´¥ç«™ç‚¹åˆ—è¡¨: {', '.join(failed_sites)}")

        self.safe_print(f"{'ğŸ“ˆ ' + '='*50 + ' ğŸ“ˆ'}")

        self.log_message("[å®Œæˆ] æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæ¯•", step="ç»“æœæŠ¥å‘Š")

    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        self.log_message("[å¼€å§‹] ç«™ç‚¹URLæµ‹è¯•å™¨å¯åŠ¨...", step="ä¸»ç¨‹åº")

        # 1. æå–URL
        extracted_urls = self.extract_urls_from_tvbox()
        if not extracted_urls:
            self.log_message("[é”™è¯¯] æœªæå–åˆ°ä»»ä½•URLï¼Œç¨‹åºé€€å‡º", step="ä¸»ç¨‹åº")
            return

        # 2. æµ‹è¯•URLå¹¶é€‰æ‹©æœ€ä¼˜
        results = {}
        for site_name, urls in extracted_urls.items():
            self.log_message(f"[å¼€å§‹] å¼€å§‹æµ‹è¯•ç«™ç‚¹: {site_name}", site_name, "ç«™ç‚¹æµ‹è¯•")

            if len(urls) == 1:
                # å•ä¸ªURLç›´æ¥æµ‹è¯•
                latency, has_keyword = self.test_url_availability(urls[0], site_name)
                if latency is not None:
                    results[site_name] = {
                        'best_url': urls[0],
                        'url_results': {urls[0]: (latency, has_keyword, self.config['default_weight'])}
                    }
                else:
                    results[site_name] = {'best_url': None}
            else:
                # å¤šä¸ªURLé€‰æ‹©æœ€ä¼˜
                best_result = self.select_best_url(urls, site_name)
                if best_result and len(best_result) == 2:
                    best_url, url_results = best_result
                    results[site_name] = {
                        'best_url': best_url,
                        'url_results': url_results
                    }
                else:
                    results[site_name] = {'best_url': None}

        # 3. ä¿å­˜JSONç»“æœå’Œæ‰“å°ç»“æœ
        self.save_results_to_json(results)
        self.print_test_results(results)

        self.log_message("[å®Œæˆ] ç«™ç‚¹URLæµ‹è¯•å®Œæˆ", step="ä¸»ç¨‹åº")
        return results

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ç«™ç‚¹URLæµ‹è¯•å™¨')
    parser.add_argument('--config', default='config/url_tester_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    tester = SiteURLTester(args.config)
    tester.run()
