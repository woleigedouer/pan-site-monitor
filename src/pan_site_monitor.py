#!/usr/bin/env python3
"""
Pan Site Monitor - ç»Ÿä¸€çš„TVBoxèµ„æºç«™ç‚¹ç›‘æ§å·¥å…·
æ”¯æŒTVBoxèµ„æºç®¡ç†ã€URLæµ‹è¯•å’ŒGitHubä¸Šä¼ åŠŸèƒ½
"""
import json
import requests
import base64
import os
import zipfile
import shutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import argparse
import sys

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

# SSLè­¦å‘Šå¤„ç†å°†åœ¨é…ç½®åŠ è½½ååŠ¨æ€è®¾ç½®


class PanSiteMonitor:
    """ç»Ÿä¸€çš„ç«™ç‚¹ç›‘æ§å·¥å…·"""
    
    def __init__(self, config_file: str = None):
        """åˆå§‹åŒ–ç›‘æ§å·¥å…·"""
        self.base_dir = Path(__file__).parent.parent.absolute()
        self.config = self._load_unified_config(config_file)
        self.last_site = None
        self.session = requests.Session()  # å¤ç”¨è¿æ¥
        
    def _load_unified_config(self, config_file: str = None):
        """åŠ è½½ç»Ÿä¸€é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒJSONå’ŒYAMLæ ¼å¼"""
        if config_file is None:
            # ä¼˜å…ˆå°è¯•YAMLæ ¼å¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨JSONæ ¼å¼
            yaml_config = self.base_dir / "config" / "app_config.yml"
            json_config = self.base_dir / "config" / "app_config.json"

            if yaml_config.exists() and YAML_AVAILABLE:
                config_file = yaml_config
                print(f"ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶: {config_file}")
            else:
                config_file = json_config
                if not yaml_config.exists():
                    print(f"YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨JSONé…ç½®æ–‡ä»¶: {config_file}")
                elif not YAML_AVAILABLE:
                    print(f"PyYAMLæœªå®‰è£…ï¼Œä½¿ç”¨JSONé…ç½®æ–‡ä»¶: {config_file}")
        else:
            config_file = Path(config_file)
            if not config_file.is_absolute():
                config_file = self.base_dir / config_file
        
        # åŠ è½½.envæ–‡ä»¶
        self._load_env_file()
        
        # åŠ è½½é…ç½®
        config = self._load_config_file(str(config_file))
        
        # åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
        self._apply_env_overrides(config)
        
        # è½¬æ¢è·¯å¾„
        self._resolve_paths(config)

        # é…ç½®SSLè­¦å‘Šå¤„ç†
        self._configure_ssl_warnings(config)

        # éªŒè¯é…ç½®å®Œæ•´æ€§
        self._validate_config(config)

        return config
    
    def _load_env_file(self):
        """åŠ è½½.envæ–‡ä»¶"""
        env_file = self.base_dir / ".env"
        if env_file.exists():
            try:
                with open(env_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            key = key.strip()
                            value = value.strip()

                            # å®‰å…¨åœ°ç§»é™¤é…å¯¹çš„å¼•å·
                            if len(value) >= 2:
                                if (value.startswith('"') and value.endswith('"')) or \
                                   (value.startswith("'") and value.endswith("'")):
                                    value = value[1:-1]

                            if key and value:
                                os.environ[key] = value
                print(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")
            except Exception as e:
                print(f"åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")
    
    def _load_config_file(self, config_file: str):
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒJSONå’ŒYAMLæ ¼å¼"""
        # æœ€å°é»˜è®¤é…ç½®ç»“æ„
        default_config = {
            "sites": {"mapping": {}, "search_paths": {}, "keyword_validation": {}},
            "tvbox": {"local_json_dir": "", "output_path": "", "version_file": "",
                     "download_path": "", "extract_path": "", "old_path": "", "api_timeout": 10,
                     "download_timeout": 60, "download_chunk_size": 8192},
            "url_tester": {"test_timeout": 15,
                          "proxy": {"enabled": False, "proxies": {}}, "history_limit": 24},
            "github": {"owner": "", "repo": "", "branch": "main", "token": "",
                      "files_to_upload": [], "commit_message_template": "Update - {timestamp}",
                      "api_timeout": 30},
            "security": {"verify_ssl": True, "ignore_ssl_warnings": False, "log_sensitive_info": False},
            "logging": {"level": "INFO", "files": {}}
        }

        try:
            if os.path.exists(config_file):
                config_path = Path(config_file)
                is_yaml = config_path.suffix.lower() in ['.yml', '.yaml']

                with open(config_file, 'r', encoding='utf-8') as f:
                    if is_yaml and YAML_AVAILABLE:
                        user_config = yaml.safe_load(f)
                        print(f"å·²åŠ è½½YAMLé…ç½®æ–‡ä»¶: {config_file}")
                    else:
                        user_config = json.load(f)
                        print(f"å·²åŠ è½½JSONé…ç½®æ–‡ä»¶: {config_file}")

                    self._deep_merge(default_config, user_config)
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                os.makedirs(os.path.dirname(config_file), exist_ok=True)
                config_path = Path(config_file)
                is_yaml = config_path.suffix.lower() in ['.yml', '.yaml']

                with open(config_file, 'w', encoding='utf-8') as f:
                    if is_yaml and YAML_AVAILABLE:
                        yaml.dump(default_config, f, default_flow_style=False,
                                allow_unicode=True, indent=2)
                        print(f"å·²åˆ›å»ºé»˜è®¤YAMLé…ç½®æ–‡ä»¶: {config_file}")
                    else:
                        json.dump(default_config, f, ensure_ascii=False, indent=2)
                        print(f"å·²åˆ›å»ºé»˜è®¤JSONé…ç½®æ–‡ä»¶: {config_file}")

                print("è­¦å‘Šï¼šé…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ç«™ç‚¹é…ç½®æ•°æ®ï¼Œç¨‹åºå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
                if is_yaml:
                    print("è¯·åœ¨config/app_config.ymlä¸­é…ç½®sitesèŠ‚çš„ç›¸å…³ä¿¡æ¯")
                else:
                    print("è¯·åœ¨config/app_config.jsonä¸­é…ç½®sitesèŠ‚çš„ç›¸å…³ä¿¡æ¯")
                print("æ³¨æ„ï¼šè¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚GitHub tokenï¼‰")
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config

    def _get_config_file_hint(self):
        """è·å–é…ç½®æ–‡ä»¶æç¤ºä¿¡æ¯"""
        yaml_config = self.base_dir / "config" / "app_config.yml"
        if yaml_config.exists() and YAML_AVAILABLE:
            return "config/app_config.yml"
        else:
            return "config/app_config.json"

    def _deep_merge(self, target, source):
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        for key, value in source.items():
            if key in target and isinstance(target[key], dict) and isinstance(value, dict):
                self._deep_merge(target[key], value)
            else:
                target[key] = value
    
    def _apply_env_overrides(self, config):
        """åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–"""
        # GitHubé…ç½®ç¯å¢ƒå˜é‡
        github_config = config.get('github', {})
        env_mappings = {
            'GITHUB_TOKEN': 'token',
            'GITHUB_OWNER': 'owner',
            'GITHUB_REPO': 'repo',
            'GITHUB_BRANCH': 'branch'
        }
        
        for env_var, config_key in env_mappings.items():
            env_value = os.getenv(env_var)
            if env_value:
                github_config[config_key] = env_value
                print(f"å·²ä»ç¯å¢ƒå˜é‡ {env_var} åŠ è½½é…ç½®")
        
        # å…¶ä»–ç¯å¢ƒå˜é‡
        if os.getenv('GITHUB_API_TIMEOUT'):
            try:
                github_config['api_timeout'] = int(os.getenv('GITHUB_API_TIMEOUT'))
            except ValueError:
                print("è­¦å‘Šï¼šGITHUB_API_TIMEOUT ç¯å¢ƒå˜é‡å€¼æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        if os.getenv('LOG_LEVEL'):
            config['logging']['level'] = os.getenv('LOG_LEVEL')
    
    def _resolve_paths(self, config):
        """è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„"""
        def resolve_path_in_dict(d, base_dir):
            for key, value in d.items():
                if isinstance(value, dict):
                    resolve_path_in_dict(value, base_dir)
                elif isinstance(value, str) and (key.endswith('_path') or key.endswith('_dir') or key.endswith('_file')):
                    # æ’é™¤è¿œç¨‹æ–‡ä»¶åï¼Œåªå¤„ç†æœ¬åœ°è·¯å¾„
                    if key == 'gitee_zip_file':
                        continue
                    # æ£€æŸ¥è·¯å¾„å€¼æ˜¯å¦æœ‰æ•ˆ
                    if value and value.strip() and not os.path.isabs(value):
                        d[key] = str(base_dir / value)

        resolve_path_in_dict(config, self.base_dir)

    def _configure_ssl_warnings(self, config):
        """é…ç½®SSLè­¦å‘Šå¤„ç†"""
        security_config = config.get('security', {})

        if not security_config.get('ignore_ssl_warnings', False):
            # å¦‚æœä¸å¿½ç•¥SSLè­¦å‘Šï¼Œåˆ™ä¸åšä»»ä½•å¤„ç†ï¼ˆä¿æŒé»˜è®¤è¡Œä¸ºï¼‰
            pass
        else:
            # åªæœ‰åœ¨é…ç½®æ˜ç¡®è¦æ±‚æ—¶æ‰å¿½ç•¥SSLè­¦å‘Š
            try:
                import warnings
                from urllib3.exceptions import InsecureRequestWarning
                warnings.simplefilter('ignore', InsecureRequestWarning)
                print("è­¦å‘Šï¼šå·²ç¦ç”¨SSLè¯ä¹¦éªŒè¯è­¦å‘Š")
            except ImportError:
                print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥urllib3ï¼ŒSSLè­¦å‘Šé…ç½®å¯èƒ½æ— æ•ˆ")

    def _sanitize_config_for_logging(self, config):
        """æ¸…ç†é…ç½®ä¸­çš„æ•æ„Ÿä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•"""
        if not config.get('security', {}).get('log_sensitive_info', False):
            sanitized = json.loads(json.dumps(config))  # æ·±æ‹·è´

            # æ¸…ç†GitHub token
            if 'github' in sanitized and 'token' in sanitized['github']:
                token = sanitized['github']['token']
                if token and len(token) > 8:
                    sanitized['github']['token'] = f"{token[:4]}...{token[-4:]}"
                elif token:
                    sanitized['github']['token'] = "***"

            # æ¸…ç†å…¶ä»–å¯èƒ½çš„æ•æ„Ÿä¿¡æ¯
            if 'url_tester' in sanitized and 'proxy' in sanitized['url_tester']:
                proxy_config = sanitized['url_tester']['proxy']
                if 'proxies' in proxy_config:
                    for key, value in proxy_config['proxies'].items():
                        if '@' in str(value):  # å¯èƒ½åŒ…å«ç”¨æˆ·åå¯†ç çš„ä»£ç†
                            sanitized['url_tester']['proxy']['proxies'][key] = "***"

            return sanitized
        else:
            return config

    def _validate_config(self, config):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        sites_config = config.get('sites', {})

        # æ£€æŸ¥ç«™ç‚¹é…ç½®æ˜¯å¦ä¸ºç©º
        mapping = sites_config.get('mapping', {})
        search_paths = sites_config.get('search_paths', {})
        keyword_validation = sites_config.get('keyword_validation', {})

        if not mapping:
            print("è­¦å‘Šï¼šç«™ç‚¹æ˜ å°„é…ç½®ä¸ºç©ºï¼ŒTVBoxåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
            print(f"æç¤ºï¼šè¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®sites.mapping")

        if not search_paths:
            print("è­¦å‘Šï¼šæœç´¢è·¯å¾„é…ç½®ä¸ºç©ºï¼ŒURLæµ‹è¯•åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
            print(f"æç¤ºï¼šè¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®sites.search_paths")

        if not keyword_validation:
            print("è­¦å‘Šï¼šå…³é”®å­—éªŒè¯é…ç½®ä¸ºç©ºï¼ŒURLæµ‹è¯•çš„å‡†ç¡®æ€§å¯èƒ½å—å½±å“")
            print(f"æç¤ºï¼šè¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®sites.keyword_validation")

        # æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
        mapping_sites = set(mapping.values())
        search_sites = set(search_paths.keys())
        keyword_sites = set(keyword_validation.keys())

        missing_search = mapping_sites - search_sites
        missing_keyword = mapping_sites - keyword_sites

        if missing_search:
            print(f"è­¦å‘Šï¼šä»¥ä¸‹ç«™ç‚¹ç¼ºå°‘æœç´¢è·¯å¾„é…ç½®: {', '.join(missing_search)}")

        if missing_keyword:
            print(f"è­¦å‘Šï¼šä»¥ä¸‹ç«™ç‚¹ç¼ºå°‘å…³é”®å­—éªŒè¯é…ç½®: {', '.join(missing_keyword)}")

        # æ£€æŸ¥å…¶ä»–å¿…è¦é…ç½®
        if not config.get('github', {}).get('token') or config.get('github', {}).get('token', '').startswith('è¯·è®¾ç½®'):
            print("æç¤ºï¼šGitHub tokenæœªé…ç½®ï¼ŒGitHubä¸Šä¼ åŠŸèƒ½å°†ä¸å¯ç”¨")

        # æ£€æŸ¥TVBoxè·¯å¾„é…ç½®
        self._validate_tvbox_paths(config)

        # æ£€æŸ¥TVBox Giteeé…ç½®
        self._validate_tvbox_gitee_config(config)

    def _validate_tvbox_paths(self, config):
        """éªŒè¯TVBoxè·¯å¾„é…ç½®çš„åˆç†æ€§"""
        tvbox_config = config.get('tvbox', {})
        extract_path = tvbox_config.get('extract_path', '')
        old_path = tvbox_config.get('old_path', '')

        if extract_path and old_path:
            extract_abs = os.path.abspath(extract_path)
            old_abs = os.path.abspath(old_path)

            # æ£€æŸ¥old_pathæ˜¯å¦åœ¨extract_pathå†…éƒ¨
            if old_abs.startswith(extract_abs + os.sep) or old_abs == extract_abs:
                print(f"é”™è¯¯ï¼šTVBoxé…ç½®ä¸­çš„å¤‡ä»½è·¯å¾„ '{old_path}' ä¸èƒ½åœ¨è§£å‹è·¯å¾„ '{extract_path}' å†…éƒ¨")
                print("å»ºè®®ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ tvbox.old_path ä¸ºç‹¬ç«‹çš„ç›®å½•ï¼Œä¾‹å¦‚ï¼š")
                print(f"  \"old_path\": \"files_backup\" æˆ– \"old_path\": \"backup/files\"")
                raise ValueError(f"TVBoxè·¯å¾„é…ç½®é”™è¯¯ï¼šå¤‡ä»½è·¯å¾„ä¸èƒ½æ˜¯è§£å‹è·¯å¾„çš„å­ç›®å½•")

    def _validate_tvbox_gitee_config(self, config):
        """éªŒè¯TVBox Giteeé…ç½®çš„å®Œæ•´æ€§"""
        tvbox_config = config.get('tvbox', {})

        required_gitee_fields = [
            'gitee_repo_owner',
            'gitee_repo_name',
            'gitee_branch',
            'gitee_zip_file'
        ]

        missing_fields = []
        for field in required_gitee_fields:
            if not tvbox_config.get(field):
                missing_fields.append(field)

        if missing_fields:
            print(f"é”™è¯¯ï¼šTVBox Giteeé…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_fields)}")
            print(f"æç¤ºï¼šè¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®ä»¥ä¸‹å­—æ®µï¼š")
            for field in missing_fields:
                print(f"  tvbox.{field}")
            raise ValueError(f"TVBox Giteeé…ç½®ä¸å®Œæ•´")

    def _setup_logging(self, module_name: str):
        """è®¾ç½®æ—¥å¿—"""
        try:
            log_level_str = self.config['logging']['level'].upper()
            log_level = getattr(logging, log_level_str)
        except (AttributeError, KeyError):
            print(f"è­¦å‘Šï¼šæ— æ•ˆçš„æ—¥å¿—çº§åˆ«ï¼Œä½¿ç”¨é»˜è®¤çº§åˆ« INFO")
            log_level = logging.INFO

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        try:
            log_file = str(self.base_dir / self.config['logging']['files'][module_name])
        except KeyError:
            log_file = str(self.base_dir / f"logs/{module_name}.log")

        os.makedirs(os.path.dirname(log_file), exist_ok=True)

        # è·å–æˆ–åˆ›å»ºlogger
        logger = logging.getLogger(f"pan_monitor_{module_name}")

        # å¦‚æœloggerå·²ç»æœ‰å¤„ç†å™¨ï¼Œç›´æ¥è¿”å›
        if logger.handlers:
            return logger

        logger.setLevel(log_level)

        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

        return logger

    def safe_log_config(self, config_section: str = None):
        """å®‰å…¨åœ°è®°å½•é…ç½®ä¿¡æ¯ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰"""
        if config_section:
            config_to_log = {config_section: self.config.get(config_section, {})}
        else:
            config_to_log = self.config

        sanitized_config = self._sanitize_config_for_logging(config_to_log)
        print(f"å½“å‰é…ç½®: {json.dumps(sanitized_config, ensure_ascii=False, indent=2)}")

    def safe_print(self, text: str):
        """å®‰å…¨æ‰“å°å‡½æ•°ï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
        try:
            print(text)
        except UnicodeEncodeError:
            safe_text = text.encode('ascii', 'ignore').decode('ascii')
            print(safe_text)

    def _safe_extract_zip(self, zip_ref: zipfile.ZipFile, extract_path: str):
        """å®‰å…¨è§£å‹ZIPæ–‡ä»¶ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»"""
        import os.path

        for member in zip_ref.infolist():
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«å±é™©è·¯å¾„
            if os.path.isabs(member.filename) or ".." in member.filename:
                raise ValueError(f"ä¸å®‰å…¨çš„ZIPæ–‡ä»¶è·¯å¾„: {member.filename}")

            # ç¡®ä¿è§£å‹åçš„è·¯å¾„åœ¨ç›®æ ‡ç›®å½•å†…
            target_path = os.path.join(extract_path, member.filename)
            target_path = os.path.normpath(target_path)

            if not target_path.startswith(os.path.normpath(extract_path)):
                raise ValueError(f"ZIPæ–‡ä»¶åŒ…å«è·¯å¾„éå†æ”»å‡»: {member.filename}")

        # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½å®‰å…¨ï¼Œåˆ™è¿›è¡Œè§£å‹
        zip_ref.extractall(extract_path)

    # ==================== TVBoxç®¡ç†åŠŸèƒ½ ====================

    def tvbox_check_version_update(self):
        """æ£€æŸ¥TVBoxç‰ˆæœ¬æ›´æ–° - åŸºäºGiteeä»“åº“æäº¤å“ˆå¸Œ"""
        logger = self._setup_logging('tvbox_manager')
        logger.info("æ£€æŸ¥TVBoxç‰ˆæœ¬æ›´æ–°")

        try:
            # ä»é…ç½®æ„å»ºGitee URL
            repo_owner = self.config['tvbox']['gitee_repo_owner']
            repo_name = self.config['tvbox']['gitee_repo_name']
            branch = self.config['tvbox']['gitee_branch']
            zip_file = self.config['tvbox']['gitee_zip_file']

            # å¯¹æ–‡ä»¶åè¿›è¡ŒURLç¼–ç ï¼Œé¿å…ä¸­æ–‡å­—ç¬¦é—®é¢˜
            from urllib.parse import quote
            encoded_zip_file = quote(zip_file, safe='')

            gitee_zip_url = f"https://gitee.com/{repo_owner}/{repo_name}/raw/{branch}/{encoded_zip_file}"
            gitee_api_url = f"https://gitee.com/api/v5/repos/{repo_owner}/{repo_name}/commits/{branch}"

            logger.debug(f"æ„å»ºçš„ä¸‹è½½URL: {gitee_zip_url}")
            logger.debug(f"æ„å»ºçš„API URL: {gitee_api_url}")

            verify_ssl = self.config.get('security', {}).get('verify_ssl', True)

            # è°ƒç”¨Gitee APIè·å–æœ€æ–°æäº¤ä¿¡æ¯
            response = requests.get(
                gitee_api_url,
                timeout=self.config['tvbox']['api_timeout'],
                verify=verify_ssl
            )
            response.raise_for_status()

            # è§£æAPIå“åº”
            commit_data = response.json()
            remote_commit_sha = commit_data.get('sha')
            remote_commit_date = commit_data.get('commit', {}).get('committer', {}).get('date')

            if not remote_commit_sha:
                logger.error("APIå“åº”ä¸­ç¼ºå°‘æäº¤SHAä¿¡æ¯")
                return 'error', None, None

            # æ ¼å¼åŒ–æäº¤æ—¶é—´æ˜¾ç¤º
            commit_time_str = ""
            if remote_commit_date:
                try:
                    # Gitee APIè¿”å›çš„æ—¶é—´æ ¼å¼: "2024-01-01T12:00:00+08:00" æˆ– "2024-01-01T12:00:00Z"
                    from datetime import datetime

                    # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
                    if remote_commit_date.endswith('Z'):
                        # UTCæ—¶é—´æ ¼å¼
                        commit_datetime = datetime.fromisoformat(remote_commit_date.replace('Z', '+00:00'))
                    else:
                        # å¸¦æ—¶åŒºçš„æ—¶é—´æ ¼å¼
                        commit_datetime = datetime.fromisoformat(remote_commit_date)

                    # æ˜¾ç¤ºè¿œç¨‹æ—¶é—´å’Œæœ¬åœ°æ—¶é—´
                    remote_time_str = commit_datetime.strftime('%Y-%m-%d %H:%M:%S %Z')
                    local_time = commit_datetime.astimezone()
                    local_time_str = local_time.strftime('%Y-%m-%d %H:%M:%S')

                    commit_time_str = f" (è¿œç¨‹: {remote_time_str}, æœ¬åœ°: {local_time_str})"
                except Exception as e:
                    logger.debug(f"è§£ææäº¤æ—¶é—´å¤±è´¥: {e}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹æ—¶é—´
                    commit_time_str = f" ({remote_commit_date})"

            logger.info(f"è¿œç¨‹æœ€æ–°æäº¤SHA: {remote_commit_sha[:8]}...{commit_time_str}")

            # æ£€æŸ¥æœ¬åœ°ç‰ˆæœ¬æ–‡ä»¶ä¸­å­˜å‚¨çš„æäº¤SHA
            version_file = self.config['tvbox']['version_file']
            local_commit_sha = None

            if os.path.exists(version_file):
                try:
                    with open(version_file, 'r', encoding='utf-8') as f:
                        local_commit_sha = f.read().strip()
                except Exception as e:
                    logger.warning(f"è¯»å–æœ¬åœ°ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥: {e}")

            # æ¯”è¾ƒæäº¤SHA
            if local_commit_sha == remote_commit_sha:
                logger.info(f"å½“å‰ç‰ˆæœ¬å·²æ˜¯æœ€æ–° (SHA: {local_commit_sha[:8] if local_commit_sha else 'æœªçŸ¥'}...{commit_time_str})")
                return 'up_to_date', remote_commit_sha, gitee_zip_url
            else:
                local_short = local_commit_sha[:8] + '...' if local_commit_sha else 'æœªçŸ¥'
                remote_short = remote_commit_sha[:8] + '...'
                logger.info(f"å‘ç°æ–°ç‰ˆæœ¬: {remote_short}{commit_time_str} (å½“å‰: {local_short})")
                return 'need_update', remote_commit_sha, gitee_zip_url

        except Exception as e:
            logger.error(f"æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°å¤±è´¥: {e}")
            return 'error', None, None

    def tvbox_download_and_update(self, commit_sha: str, url: str):
        """ä¸‹è½½å¹¶æ›´æ–°TVBoxèµ„æº - ä»Giteeä¸‹è½½å›ºå®šZIPæ–‡ä»¶"""
        logger = self._setup_logging('tvbox_manager')
        logger.info(f"å¼€å§‹ä¸‹è½½æ›´æ–° (æäº¤SHA: {commit_sha[:8]}...)")

        try:
            # ä¸‹è½½æ–‡ä»¶
            logger.debug(f"å‡†å¤‡ä¸‹è½½URL: {url}")
            verify_ssl = self.config.get('security', {}).get('verify_ssl', True)
            response = requests.get(url, timeout=self.config['tvbox']['download_timeout'], stream=True, verify=verify_ssl)
            response.raise_for_status()

            download_path = self.config['tvbox']['download_path']
            os.makedirs(os.path.dirname(download_path), exist_ok=True)

            with open(download_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=self.config['tvbox']['download_chunk_size']):
                    if chunk:
                        f.write(chunk)

            logger.info(f"ä¸‹è½½å®Œæˆ: {download_path}")

            # è§£å‹æ–‡ä»¶ï¼ˆåŸå­æ€§æ“ä½œï¼Œç¡®ä¿æ•°æ®å®‰å…¨ï¼‰
            extract_path = self.config['tvbox']['extract_path']
            old_path = self.config['tvbox']['old_path']
            temp_extract_path = extract_path + "_temp"

            try:
                # å…ˆè§£å‹åˆ°ä¸´æ—¶ç›®å½•
                if os.path.exists(temp_extract_path):
                    shutil.rmtree(temp_extract_path)

                os.makedirs(temp_extract_path, exist_ok=True)

                # è§£å‹æ–°æ–‡ä»¶ï¼ˆå®‰å…¨è§£å‹ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼‰
                with zipfile.ZipFile(download_path, 'r') as zip_ref:
                    self._safe_extract_zip(zip_ref, temp_extract_path)

                logger.info(f"è§£å‹åˆ°ä¸´æ—¶ç›®å½•å®Œæˆ: {temp_extract_path}")

                # å¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if os.path.exists(extract_path):
                    if os.path.exists(old_path):
                        shutil.rmtree(old_path)
                    shutil.move(extract_path, old_path)
                    logger.info(f"å·²å¤‡ä»½ç°æœ‰æ–‡ä»¶åˆ°: {old_path}")

                # å°†ä¸´æ—¶ç›®å½•ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
                shutil.move(temp_extract_path, extract_path)
                logger.info(f"è§£å‹å®Œæˆ: {extract_path}")

            except Exception as e:
                # å¦‚æœæ“ä½œå¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶å¹¶æ¢å¤å¤‡ä»½
                if os.path.exists(temp_extract_path):
                    shutil.rmtree(temp_extract_path)

                if os.path.exists(old_path) and not os.path.exists(extract_path):
                    shutil.move(old_path, extract_path)
                    logger.info("å·²æ¢å¤å¤‡ä»½æ–‡ä»¶")

                raise e

            # æ›´æ–°ç‰ˆæœ¬æ–‡ä»¶ï¼Œä¿å­˜æäº¤SHA
            version_file = self.config['tvbox']['version_file']
            os.makedirs(os.path.dirname(version_file), exist_ok=True)
            with open(version_file, 'w', encoding='utf-8') as f:
                f.write(commit_sha)

            logger.info(f"ç‰ˆæœ¬æ›´æ–°å®Œæˆï¼Œæäº¤SHAå·²ä¿å­˜: {commit_sha[:8]}...")
            return True

        except Exception as e:
            logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
            return False

    def tvbox_aggregate_data(self):
        """èšåˆTVBoxæ•°æ®"""
        logger = self._setup_logging('tvbox_manager')
        logger.info("å¼€å§‹èšåˆTVBoxæ•°æ®")

        try:
            local_data = {}
            success_count = 0

            # æ£€æŸ¥ç«™ç‚¹æ˜ å°„é…ç½®
            site_mapping = self.config['sites']['mapping']
            if not site_mapping:
                logger.warning("ç«™ç‚¹æ˜ å°„é…ç½®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ•°æ®èšåˆ")
                logger.info(f"è¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®sites.mapping")
                return False

            total_count = len(site_mapping)
            json_dir = self.config['tvbox']['local_json_dir']

            for filename, site_name in site_mapping.items():
                file_path = os.path.join(json_dir, filename)

                try:
                    if os.path.exists(file_path):
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        if 'Domains' in data and isinstance(data['Domains'], list):
                            urls = []
                            for domain_info in data['Domains']:
                                if isinstance(domain_info, dict) and 'url' in domain_info:
                                    urls.append(domain_info['url'])
                                elif isinstance(domain_info, str):
                                    urls.append(domain_info)

                            if urls:
                                local_data[site_name] = urls
                                success_count += 1
                                logger.info(f"å¤„ç†æˆåŠŸ: {filename} -> {site_name} ({len(urls)} ä¸ªURL)")
                            else:
                                logger.warning(f"æ–‡ä»¶ {filename} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL")
                        else:
                            logger.warning(f"æ–‡ä»¶ {filename} æ ¼å¼æ— æ•ˆ")
                    else:
                        logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue

            # ä¿å­˜èšåˆæ•°æ®
            output_path = self.config['tvbox']['output_path']
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(local_data, f, ensure_ascii=False, indent=2)

            logger.info(f"æ•°æ®èšåˆå®Œæˆ: {success_count}/{total_count} ä¸ªæ–‡ä»¶å¤„ç†æˆåŠŸ")
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")

            return success_count > 0

        except Exception as e:
            logger.error(f"æ•°æ®èšåˆå¤±è´¥: {e}")
            return False

    def run_tvbox_manager(self, check_update: bool = True, aggregate_data: bool = True):
        """è¿è¡ŒTVBoxç®¡ç†å™¨"""
        logger = self._setup_logging('tvbox_manager')
        logger.info("TVBoxç®¡ç†å™¨å¯åŠ¨")

        results = {"update": False, "aggregate": False}

        # æ£€æŸ¥å¹¶æ›´æ–°ç‰ˆæœ¬
        status = None
        if check_update:
            status, commit_sha, url = self.tvbox_check_version_update()

            if status == 'error':
                logger.error("ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡åç»­æ“ä½œ")
                return results
            elif status == 'need_update':
                if commit_sha and url:
                    logger.info("å‘ç°æ–°ç‰ˆæœ¬ï¼Œå¼€å§‹æ›´æ–°...")
                    results["update"] = self.tvbox_download_and_update(commit_sha, url)
                    if not results["update"]:
                        logger.error("ç‰ˆæœ¬æ›´æ–°å¤±è´¥ï¼Œè·³è¿‡æ•°æ®èšåˆ")
                        return results
                else:
                    logger.error("ç‰ˆæœ¬ä¿¡æ¯ä¸å®Œæ•´ï¼Œè·³è¿‡æ›´æ–°")
                    return results
            elif status == 'up_to_date':
                logger.info("å½“å‰å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")
                results["update"] = True
        else:
            # å¦‚æœè·³è¿‡ç‰ˆæœ¬æ£€æŸ¥ï¼ŒéªŒè¯æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            json_dir = self.config['tvbox']['local_json_dir']
            if not os.path.exists(json_dir):
                logger.warning("è·³è¿‡ç‰ˆæœ¬æ£€æŸ¥ä½†æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå»ºè®®å…ˆè¿è¡Œç‰ˆæœ¬æ›´æ–°")
            results["update"] = True
            status = 'skip_check'  # æ ‡è®°ä¸ºè·³è¿‡æ£€æŸ¥

        # èšåˆæ•°æ® - åªæœ‰å½“ç‰ˆæœ¬å®é™…æ›´æ–°æˆ–è·³è¿‡æ£€æŸ¥æ—¶æ‰èšåˆ
        if aggregate_data and results["update"]:
            if status == 'need_update' or status == 'skip_check':
                results["aggregate"] = self.tvbox_aggregate_data()
            elif status == 'up_to_date':
                logger.info("ç‰ˆæœ¬æ— æ›´æ–°ï¼Œè·³è¿‡æ•°æ®èšåˆ")
                results["aggregate"] = True  # æ ‡è®°ä¸ºæˆåŠŸï¼Œä½†å®é™…è·³è¿‡

        logger.info(f"TVBoxç®¡ç†å™¨å®Œæˆ: æ›´æ–°={results['update']}, èšåˆ={results['aggregate']}")
        return results

    # ==================== URLæµ‹è¯•åŠŸèƒ½ ====================

    def log_message(self, message, site_name=None, step=""):
        """æ ¼å¼åŒ–æ‰“å°æ—¥å¿—æ¶ˆæ¯"""
        status_emojis = {
            '[å¼€å§‹]': 'ğŸš€', '[æˆåŠŸ]': 'âœ…', '[å®Œæˆ]': 'ğŸ‰', '[å¤±è´¥]': 'âŒ',
            '[è¶…æ—¶]': 'â³', '[è­¦å‘Š]': 'âš ï¸', '[é”™è¯¯]': 'ğŸš¨', '[ä¿¡æ¯]': 'â„¹ï¸',
            '[é€‰æ‹©]': 'ğŸ”', '[è¿æ¥å¤±è´¥]': 'ğŸ”Œ'
        }

        if site_name and site_name != self.last_site:
            self.safe_print(f"\n{'âœ¨ ' + '='*38 + ' âœ¨'}\nğŸŒ [ç«™ç‚¹: {site_name}]\n{'âœ¨ ' + '='*38 + ' âœ¨'}")
            self.last_site = site_name

        for status, emoji in status_emojis.items():
            if status in message:
                message = message.replace(status, emoji)
                break

        if step:
            self.safe_print(f"ğŸ“‹ [{step}] {message}")
        else:
            self.safe_print(f"    {message}")

    def extract_urls_from_sources(self):
        """ä»æ•°æ®æºæå–URL"""
        self.log_message("[å¼€å§‹] å¼€å§‹æå–URLä¿¡æ¯", step="æå–URL")
        extracted_urls = {}

        # é¦–å…ˆå°è¯•ä»data/test.jsonè¯»å–ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        test_json_path = self.base_dir / "data" / "test.json"
        if test_json_path.exists():
            self.log_message("[ä¿¡æ¯] å‘ç°data/test.jsonï¼Œä½¿ç”¨æ­¤æ–‡ä»¶ä½œä¸ºæ•°æ®æº", step="æå–URL")
            try:
                with open(test_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # data/test.jsonæ ¼å¼: {"ç«™ç‚¹å": ["url1", "url2", ...]}
                for site_name, urls in data.items():
                    if isinstance(urls, list) and urls:
                        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²å’Œæ— æ•ˆURL
                        valid_urls = [url.strip() for url in urls if url and url.strip()]
                        if valid_urls:
                            extracted_urls[site_name] = valid_urls
                            self.log_message(f"[æˆåŠŸ] {site_name}: æ‰¾åˆ° {len(valid_urls)} ä¸ªURL",
                                           site_name, "æå–URL")
                        else:
                            self.log_message(f"[è­¦å‘Š] {site_name} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL", site_name, "æå–URL")
                    else:
                        self.log_message(f"[è­¦å‘Š] {site_name} çš„URLåˆ—è¡¨æ ¼å¼æ— æ•ˆ", site_name, "æå–URL")

                self.log_message(f"[å®Œæˆ] ä»data/test.jsonå…±æå–åˆ° {len(extracted_urls)} ä¸ªç«™ç‚¹çš„URLä¿¡æ¯", step="æå–URL")
                return extracted_urls

            except Exception as e:
                self.log_message(f"[é”™è¯¯] è¯»å–data/test.jsonå¤±è´¥: {e}", step="æå–URL")
                # ç»§ç»­å°è¯•TVBoxç›®å½•

        # å¦‚æœdata/test.jsonä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå°è¯•ä»TVBoxç›®å½•è¯»å–
        tvbox_dir = self.config['tvbox']['local_json_dir']

        if not os.path.exists(tvbox_dir):
            self.log_message(f"[é”™è¯¯] TVBoxç›®å½•ä¸å­˜åœ¨: {tvbox_dir}ï¼Œä¸”data/test.jsonä¹Ÿä¸å¯ç”¨", step="æå–URL")
            return {}

        self.log_message(f"[ä¿¡æ¯] ä»TVBoxç›®å½•è¯»å–: {tvbox_dir}", step="æå–URL")

        # æ£€æŸ¥ç«™ç‚¹æ˜ å°„é…ç½®
        site_mapping = self.config['sites']['mapping']
        if not site_mapping:
            self.log_message("[é”™è¯¯] ç«™ç‚¹æ˜ å°„é…ç½®ä¸ºç©ºï¼Œæ— æ³•å¤„ç†TVBoxæ–‡ä»¶", step="æå–URL")
            self.log_message(f"[ä¿¡æ¯] è¯·åœ¨{self._get_config_file_hint()}ä¸­é…ç½®sites.mapping", step="æå–URL")
            return {}

        for filename, site_name in site_mapping.items():
            file_path = os.path.join(tvbox_dir, filename)

            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    # æå–Domainså­—æ®µä¸­çš„URL
                    if 'Domains' in data and isinstance(data['Domains'], list):
                        urls = []
                        for domain_info in data['Domains']:
                            if isinstance(domain_info, dict) and 'url' in domain_info:
                                urls.append(domain_info['url'])
                            elif isinstance(domain_info, str):
                                urls.append(domain_info)

                        if urls:
                            extracted_urls[site_name] = urls
                            self.log_message(f"[æˆåŠŸ] {filename} -> {site_name}: æ‰¾åˆ° {len(urls)} ä¸ªURL",
                                           site_name, "æå–URL")
                        else:
                            self.log_message(f"[è­¦å‘Š] {filename} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆURL", site_name, "æå–URL")
                    else:
                        self.log_message(f"[è­¦å‘Š] {filename} ä¸­æœªæ‰¾åˆ°Domainså­—æ®µ", site_name, "æå–URL")
                else:
                    self.log_message(f"[è­¦å‘Š] æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", step="æå–URL")

            except Exception as e:
                self.log_message(f"[é”™è¯¯] å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}", step="æå–URL")
                continue

        self.log_message(f"[å®Œæˆ] å…±æå–åˆ° {len(extracted_urls)} ä¸ªç«™ç‚¹çš„URLä¿¡æ¯", step="æå–URL")
        return extracted_urls

    def test_url_availability(self, url, site_name=None):
        """æµ‹è¯•å•ä¸ªURLçš„å¯ç”¨æ€§"""
        search_path = self.config['sites'].get('search_paths', {}).get(site_name)

        # å®‰å…¨åœ°æ‹¼æ¥URLå’Œæœç´¢è·¯å¾„
        if search_path:
            from urllib.parse import urljoin
            base_url = url.strip()
            # ç¡®ä¿base_urlä»¥æ–œæ ç»“å°¾ï¼Œä»¥ä¾¿æ­£ç¡®æ‹¼æ¥
            if not base_url.endswith('/'):
                base_url += '/'
            test_url_str = urljoin(base_url, search_path.lstrip('/'))
        else:
            test_url_str = url.strip()

        keyword = self.config['sites'].get('keyword_validation', {}).get(site_name)

        # è®¾ç½®ä»£ç†
        proxies = None
        proxy_config = self.config.get('url_tester', {}).get('proxy', {})
        if proxy_config.get('enabled', False):
            proxies = proxy_config.get('proxies', {})

        # è®¾ç½®SSLéªŒè¯
        verify_ssl = self.config.get('security', {}).get('verify_ssl', True)

        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }



        # é‡è¯•æœºåˆ¶ï¼šé’ˆå¯¹403/503ç­‰ä¸´æ—¶é”™è¯¯
        max_retries = 2
        retry_delay = 1  # ç§’

        for attempt in range(max_retries + 1):
            try:
                timeout = self.config.get('url_tester', {}).get('test_timeout', 15)
                response = self.session.get(
                    test_url_str,
                    timeout=timeout,
                    verify=verify_ssl,
                    proxies=proxies,
                    headers=headers,
                    allow_redirects=True
                )
                latency = response.elapsed.total_seconds()

                if response.status_code == 200:
                    has_keyword = keyword in response.text if keyword else True
                    if has_keyword:
                        self.log_message(f"[æˆåŠŸ] URL {test_url_str} å»¶è¿Ÿ: {latency:.2f}s{'ï¼ŒåŒ…å«å…³é”®å­— ' + keyword if keyword else ''}",
                                        site_name, "æµ‹è¯•URL")
                        return latency, has_keyword, None
                    else:
                        # æ— å…³é”®å­—çš„URLè§†ä¸ºæ— æ•ˆï¼Œè¿”å›å¤±è´¥çŠ¶æ€
                        self.log_message(f"[å¤±è´¥] URL {test_url_str} å»¶è¿Ÿ: {latency:.2f}sï¼Œä½†ä¸åŒ…å«å…³é”®å­— '{keyword}'",
                                        site_name, "æµ‹è¯•URL")
                        self.log_message(f"[åˆ¤å®š] è¯¥URLè¿”å›200ä½†æ— å…³é”®å­—ï¼Œåˆ¤å®šä¸ºæ— æ•ˆï¼ˆå¯èƒ½æ˜¯åŸŸåè¿‡æœŸã€Cloudflareç›¾ç­‰ï¼‰",
                                        site_name, "æµ‹è¯•URL")
                        return None, False, {"type": "invalid_content", "detail": "æ— å…³é”®å­—å†…å®¹"}
                elif response.status_code in [403, 503, 429] and attempt < max_retries:
                    # ä¸´æ—¶é”™è¯¯ï¼Œé‡è¯•
                    import time
                    self.log_message(f"[é‡è¯•] URL {test_url_str} è¿”å›{response.status_code}ï¼Œ{retry_delay}ç§’åé‡è¯• ({attempt + 1}/{max_retries})",
                                    site_name, "æµ‹è¯•URL")
                    time.sleep(retry_delay)
                    continue
                else:
                    error_detail = f"çŠ¶æ€ç  {response.status_code}"
                    self.log_message(f"[å¤±è´¥] URL {test_url_str} è¿”å›HTTPé”™è¯¯: {error_detail}",
                                    site_name, "æµ‹è¯•URL")
                    return None, None, {"type": "http_error", "detail": error_detail}
            except requests.exceptions.Timeout:
                if attempt < max_retries:
                    import time
                    self.log_message(f"[é‡è¯•] URL {test_url_str} è¶…æ—¶ï¼Œ{retry_delay}ç§’åé‡è¯• ({attempt + 1}/{max_retries})",
                                    site_name, "æµ‹è¯•URL")
                    time.sleep(retry_delay)
                    continue
                timeout = self.config.get('url_tester', {}).get('test_timeout', 15)
                error_detail = f"è¯·æ±‚è¶…æ—¶ (>{timeout}s)"
                self.log_message(f"[è¶…æ—¶] URL {test_url_str} {error_detail}",
                                site_name, "æµ‹è¯•URL")
                return None, None, {"type": "timeout", "detail": "è¶…æ—¶"}
            except requests.exceptions.SSLError as e:
                error_detail = f"SSLé”™è¯¯: {str(e)[:100]}"
                self.log_message(f"[SSLé”™è¯¯] URL {test_url_str} {error_detail}", site_name, "æµ‹è¯•URL")
                return None, None, {"type": "ssl_error", "detail": "SSLé”™è¯¯"}
            except requests.exceptions.ConnectionError as e:
                if attempt < max_retries:
                    import time
                    self.log_message(f"[é‡è¯•] URL {test_url_str} è¿æ¥å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯• ({attempt + 1}/{max_retries})",
                                    site_name, "æµ‹è¯•URL")
                    time.sleep(retry_delay)
                    continue
                error_detail = f"è¿æ¥å¤±è´¥: {str(e)[:100]}"
                self.log_message(f"[è¿æ¥å¤±è´¥] URL {test_url_str} {error_detail}", site_name, "æµ‹è¯•URL")
                return None, None, {"type": "connection_error", "detail": "è¿æ¥å¤±è´¥"}
            except Exception as e:
                error_detail = f"æµ‹è¯•å¼‚å¸¸: {str(e)[:100]}"
                self.log_message(f"[é”™è¯¯] URL {test_url_str} {error_detail}", site_name, "æµ‹è¯•URL")
                return None, None, {"type": "unknown_error", "detail": "æœªçŸ¥é”™è¯¯"}

    def test_site_urls(self, site_name, urls):
        """æµ‹è¯•å•ä¸ªç«™ç‚¹çš„æ‰€æœ‰URL"""
        self.log_message(f"[å¼€å§‹] å¼€å§‹æµ‹è¯•ç«™ç‚¹ {site_name} çš„ {len(urls)} ä¸ªURL", site_name, "æµ‹è¯•ç«™ç‚¹")

        url_results = {}
        valid_urls = {}  # åªåŒ…å«æœ‰å…³é”®å­—çš„æœ‰æ•ˆURL

        for idx, url in enumerate(urls):
            if not url or not url.strip():
                continue

            latency, has_keyword, error_info = self.test_url_availability(url, site_name)

            if latency is not None and has_keyword:
                # æˆåŠŸï¼šæœ‰å»¶è¿Ÿä¸”åŒ…å«å…³é”®å­—çš„URLæ‰ç®—æœ‰æ•ˆ
                valid_urls[url] = latency
                url_results[url] = (latency, has_keyword, None, None)
            else:
                # å¤±è´¥ï¼šè®°å½•é”™è¯¯ä¿¡æ¯ï¼ˆåŒ…æ‹¬æ— å…³é”®å­—çš„æƒ…å†µï¼‰
                url_results[url] = (None, False, None, error_info)

            # è¯·æ±‚é—´éš”ï¼šé¿å…è§¦å‘åçˆ¬è™«ï¼ˆæœ€åä¸€ä¸ªURLä¸éœ€è¦å»¶è¿Ÿï¼‰
            if idx < len(urls) - 1:
                import time
                time.sleep(0.8)

        # é€‰æ‹©æœ‰æ•ˆURLä¸­å»¶è¿Ÿæœ€ä½çš„
        if valid_urls:
            best_url = min(valid_urls.keys(), key=lambda u: valid_urls[u])
            best_latency = valid_urls[best_url]

            self.log_message(f"[é€‰æ‹©] æœ€ä½³URL: {best_url} (å»¶è¿Ÿ: {best_latency:.2f}s, åŒ…å«å…³é”®å­—)",
                            site_name, "é€‰æ‹©æœ€ä½³")

            return {
                'best_url': best_url,
                'url_results': url_results
            }
        else:
            self.log_message(f"[å¤±è´¥] ç«™ç‚¹ {site_name} æ²¡æœ‰æœ‰æ•ˆURL", site_name, "æµ‹è¯•ç«™ç‚¹")
            return {'best_url': None, 'url_results': url_results}

    def run_url_tester(self):
        """è¿è¡ŒURLæµ‹è¯•å™¨"""
        self.log_message("[å¼€å§‹] URLæµ‹è¯•å™¨å¯åŠ¨", step="ä¸»ç¨‹åº")

        # æå–URL
        extracted_urls = self.extract_urls_from_sources()

        if not extracted_urls:
            self.log_message("[é”™è¯¯] æœªæ‰¾åˆ°ä»»ä½•URLæ•°æ®ï¼Œç¨‹åºé€€å‡º", step="ä¸»ç¨‹åº")
            return {}

        # æµ‹è¯•æ‰€æœ‰ç«™ç‚¹
        results = {}

        for site_name, urls in extracted_urls.items():
            try:
                site_result = self.test_site_urls(site_name, urls)
                results[site_name] = site_result
            except Exception as e:
                self.log_message(f"[é”™è¯¯] æµ‹è¯•ç«™ç‚¹ {site_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", site_name, "æµ‹è¯•ç«™ç‚¹")
                results[site_name] = {'best_url': None, 'url_results': {}}

        # ä¿å­˜ç»“æœ
        self.save_test_results(results)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for result in results.values() if result['best_url'])
        total_count = len(results)

        self.log_message(f"[å®Œæˆ] URLæµ‹è¯•å®Œæˆ: {success_count}/{total_count} ä¸ªç«™ç‚¹æµ‹è¯•æˆåŠŸ", step="ä¸»ç¨‹åº")
        return results

    def save_test_results(self, results):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            output_file = self.base_dir / "web" / "assets" / "data" / "test_results.json"
            os.makedirs(output_file.parent, exist_ok=True)

            # æ„å»ºJSONæ•°æ®
            json_data = {
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "total_sites": len(results),
                    "success_sites": sum(1 for result in results.values() if result['best_url']),
                    "failed_sites": sum(1 for result in results.values() if not result['best_url'])
                },
                "sites": {}
            }

            for site_name, result in results.items():
                site_data = {
                    "site_name": site_name,
                    "best_url": result['best_url'],
                    "status": "success" if result['best_url'] else "failed",
                    "urls": []
                }

                if 'url_results' in result and result['url_results']:
                    for url, url_result in result['url_results'].items():
                        # å¤„ç†æ•°æ®ç»“æ„ï¼š(latency, has_keyword, weight, error_info)
                        latency, has_keyword, weight, error_info = url_result

                        url_data = {
                            "url": url,
                            "latency": round(latency, 2) if latency is not None else None,
                            "has_keyword": has_keyword,
                            "is_best": url == result['best_url']
                        }

                        # æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if error_info:
                            url_data["error_type"] = error_info.get("type")
                            url_data["error_detail"] = error_info.get("detail")

                        site_data['urls'].append(url_data)

                    # æŒ‰æ˜¯å¦ä¸ºæœ€ä½³URLæ’åºï¼Œæœ€ä½³çš„åœ¨å‰é¢ï¼Œå¤±è´¥çš„URLæ’åœ¨æœ€å
                    site_data['urls'].sort(key=lambda x: (not x['is_best'], x['latency'] is None, x['latency'] or 999))

                json_data['sites'][site_name] = site_data

            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            self.log_message(f"[æˆåŠŸ] æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}", step="ä¿å­˜ç»“æœ")
            
            # æ›´æ–°å†å²æ•°æ®
            self.update_history(results)

        except Exception as e:
            self.log_message(f"[é”™è¯¯] ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}", step="ä¿å­˜ç»“æœ")
            
    def update_history(self, results):
        """æ›´æ–°URLå†å²çŠ¶æ€è®°å½•ï¼ˆæŒ‰ç½‘ç«™åˆ†ç±»ï¼‰
        
        å°†URLæµ‹è¯•ç»“æœä¿å­˜åˆ°å†å²è®°å½•æ–‡ä»¶(web/assets/data/history.json)ä¸­ï¼Œç”¨äºå‰ç«¯å±•ç¤ºURLçŠ¶æ€çš„å†å²å˜åŒ–ã€‚
        é‡‡ç”¨æŒ‰ç«™ç‚¹åˆ†ç±»çš„åµŒå¥—æ ¼å¼å­˜å‚¨: {"ç«™ç‚¹å": {"URL": [å†å²è®°å½•åˆ—è¡¨]}}
        æ¯ä¸ªURLæœ€å¤šä¿ç•™é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šæ•°é‡çš„æœ€æ–°å†å²è®°å½•ã€‚
        """
        try:
            history_file = self.base_dir / "web" / "assets" / "data" / "history.json"

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(history_file.parent, exist_ok=True)
            
            # è¯»å–ç°æœ‰å†å²è®°å½•
            history_data = {}
            
            if history_file.exists():
                try:
                    with open(history_file, 'r', encoding='utf-8') as f:
                        history_data = json.load(f)
                except Exception as e:
                    self.log_message(f"[è­¦å‘Š] è¯»å–å†å²æ•°æ®å¤±è´¥: {e}", step="å†å²è®°å½•")
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            timestamp = datetime.now().isoformat()
            
            # ä»é…ç½®æ–‡ä»¶è·å–å†å²è®°å½•ä¿ç•™æ•°é‡é™åˆ¶
            history_limit = self.config.get('url_tester', {}).get('history_limit', 12)
            
            # æ›´æ–°æ¯ä¸ªURLçš„å†å²è®°å½•
            for site_name, result in results.items():
                # ç¡®ä¿è¯¥ç«™ç‚¹åœ¨å†å²æ•°æ®ä¸­å­˜åœ¨
                if site_name not in history_data:
                    history_data[site_name] = {}
                
                # åªå¤„ç†URLçº§å†å²è®°å½•
                if 'url_results' in result:
                    for url, url_result in result['url_results'].items():
                        # ç¡®ä¿è¯¥URLåœ¨è¯¥ç«™ç‚¹ä¸‹å­˜åœ¨
                        if url not in history_data[site_name]:
                            history_data[site_name][url] = []
                        
                        # é™åˆ¶URLå†å²è®°å½•æ•°é‡
                        if len(history_data[site_name][url]) >= history_limit:
                            history_data[site_name][url].pop(0)
                        
                        # è·å–URLçŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯
                        if len(url_result) >= 1:
                            latency = url_result[0]  # è·å–å»¶è¿Ÿæ—¶é—´
                        else:
                            latency = None

                        # è·å–é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        error_detail = None
                        if len(url_result) >= 4 and url_result[3]:  # error_infoå­˜åœ¨
                            error_info = url_result[3]
                            error_detail = error_info.get("detail")

                        # è®°å½•URLçŠ¶æ€
                        history_record = {
                            "timestamp": timestamp,
                            "status": "up" if latency is not None else "down",
                            "latency": latency,
                            "is_best": url == result['best_url']
                        }

                        # æ·»åŠ é”™è¯¯è¯¦æƒ…ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if error_detail:
                            history_record["error_detail"] = error_detail

                        history_data[site_name][url].append(history_record)
            
            # ä¿å­˜å†å²æ•°æ®
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, ensure_ascii=False, indent=2)
            
            self.log_message(f"[æˆåŠŸ] URLå†å²è®°å½•å·²æ›´æ–°: {history_file}", step="å†å²è®°å½•")
        
        except Exception as e:
            self.log_message(f"[é”™è¯¯] æ›´æ–°å†å²è®°å½•å¤±è´¥: {e}", step="å†å²è®°å½•")

    # ==================== GitHubä¸Šä¼ åŠŸèƒ½ ====================

    def get_file_sha(self, file_path: str) -> Optional[str]:
        """è·å–GitHubä¸Šæ–‡ä»¶çš„SHAå€¼"""
        try:
            github_config = self.config.get('github', {})
            url = f"https://api.github.com/repos/{github_config['owner']}/{github_config['repo']}/contents/{file_path}"

            headers = {
                'Authorization': f"token {github_config['token']}",
                'Accept': 'application/vnd.github.v3+json'
            }

            timeout = github_config.get('api_timeout', 30)
            verify_ssl = self.config.get('security', {}).get('verify_ssl', True)
            response = requests.get(url, headers=headers, timeout=timeout, verify=verify_ssl)

            if response.status_code == 200:
                return response.json().get('sha')
            elif response.status_code == 404:
                return None
            else:
                print(f"è·å–æ–‡ä»¶SHAå¤±è´¥: {response.status_code} - {response.text}")
                return None

        except Exception as e:
            print(f"è·å–æ–‡ä»¶SHAæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def upload_file_to_github(self, local_file_path: str, github_file_path: str) -> bool:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°GitHub"""
        try:
            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            full_local_path = self.base_dir / local_file_path
            if not full_local_path.exists():
                print(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {full_local_path}")
                return False

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆGitHub APIé™åˆ¶å•ä¸ªæ–‡ä»¶æœ€å¤§100MBï¼‰
            file_size = full_local_path.stat().st_size
            max_size = 100 * 1024 * 1024  # 100MB

            if file_size > max_size:
                print(f"æ–‡ä»¶è¿‡å¤§: {file_size / (1024*1024):.2f}MBï¼Œè¶…è¿‡GitHub APIé™åˆ¶(100MB)")
                return False

            if file_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Šçš„æ–‡ä»¶ç»™å‡ºè­¦å‘Š
                print(f"è­¦å‘Šï¼šæ–‡ä»¶è¾ƒå¤§({file_size / (1024*1024):.2f}MB)ï¼Œä¸Šä¼ å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")

            # è¯»å–æ–‡ä»¶å†…å®¹å¹¶ç¼–ç 
            with open(full_local_path, 'rb') as f:
                file_content = f.read()

            content_base64 = base64.b64encode(file_content).decode('utf-8')

            # è·å–ç°æœ‰æ–‡ä»¶çš„SHAï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            file_sha = self.get_file_sha(github_file_path)

            # å‡†å¤‡APIè¯·æ±‚
            github_config = self.config.get('github', {})
            url = f"https://api.github.com/repos/{github_config['owner']}/{github_config['repo']}/contents/{github_file_path}"

            headers = {
                'Authorization': f"token {github_config['token']}",
                'Accept': 'application/vnd.github.v3+json',
                'Content-Type': 'application/json'
            }

            # ç”Ÿæˆæäº¤æ¶ˆæ¯
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            commit_template = github_config.get('commit_message_template', 'Update {filename} - {timestamp}')
            commit_message = commit_template.format(
                timestamp=timestamp,
                filename=os.path.basename(github_file_path)
            )

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                'message': commit_message,
                'content': content_base64,
                'branch': github_config.get('branch', 'main')
            }

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œéœ€è¦æä¾›SHA
            if file_sha:
                data['sha'] = file_sha
                print(f"æ›´æ–°æ–‡ä»¶: {github_file_path}")
            else:
                print(f"åˆ›å»ºæ–‡ä»¶: {github_file_path}")

            # å‘é€è¯·æ±‚
            timeout = github_config.get('api_timeout', 30)
            verify_ssl = self.config.get('security', {}).get('verify_ssl', True)
            response = requests.put(url, headers=headers, json=data, timeout=timeout, verify=verify_ssl)

            if response.status_code in [200, 201]:
                print(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {github_file_path}")
                return True
            else:
                print(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.status_code} - {response.text}")
                return False

        except Exception as e:
            print(f"ä¸Šä¼ æ–‡ä»¶ {local_file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def _validate_github_config(self, github_config: dict) -> tuple[bool, list]:
        """éªŒè¯GitHubé…ç½®çš„æœ‰æ•ˆæ€§"""
        import re

        # å®šä¹‰é…ç½®éªŒè¯è§„åˆ™
        validation_rules = {
            'owner': {
                'required': True,
                'invalid_patterns': [r'^è¯·è®¾ç½®.*', r'^your-.*', r'^<.*>$', r'^\s*$'],
                'description': 'GitHubç”¨æˆ·åæˆ–ç»„ç»‡å (ç¯å¢ƒå˜é‡: GITHUB_OWNER)'
            },
            'repo': {
                'required': True,
                'invalid_patterns': [r'^è¯·è®¾ç½®.*', r'^your-.*', r'^<.*>$', r'^\s*$'],
                'description': 'GitHubä»“åº“å (ç¯å¢ƒå˜é‡: GITHUB_REPO)'
            },
            'token': {
                'required': True,
                'invalid_patterns': [r'^è¯·è®¾ç½®.*', r'^your-.*', r'^<.*>$', r'^\s*$', r'^ghp_example.*'],
                'min_length': 10,
                'description': 'GitHubè®¿é—®ä»¤ç‰Œ (ç¯å¢ƒå˜é‡: GITHUB_TOKEN)'
            }
        }

        missing_configs = []

        for field, rules in validation_rules.items():
            value = github_config.get(field, '')

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¿…éœ€å­—æ®µ
            if rules.get('required', False) and not value:
                missing_configs.append(rules['description'])
                continue

            # æ£€æŸ¥æœ€å°é•¿åº¦
            if 'min_length' in rules and len(value) < rules['min_length']:
                missing_configs.append(f"{rules['description']} (é•¿åº¦ä¸è¶³)")
                continue

            # æ£€æŸ¥æ— æ•ˆæ¨¡å¼
            if 'invalid_patterns' in rules:
                for pattern in rules['invalid_patterns']:
                    if re.match(pattern, value, re.IGNORECASE):
                        missing_configs.append(rules['description'])
                        break

        return len(missing_configs) == 0, missing_configs

    def run_github_uploader(self):
        """è¿è¡ŒGitHubä¸Šä¼ å™¨"""
        print("GitHubä¸Šä¼ å™¨å¯åŠ¨")

        # æ£€æŸ¥é…ç½®
        github_config = self.config.get('github', {})
        is_valid, missing_configs = self._validate_github_config(github_config)

        if not is_valid:
            print(f"GitHubé…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing_configs)}")
            print("è¯·è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡æˆ–ç¼–è¾‘é…ç½®æ–‡ä»¶")
            print("æ³¨æ„ï¼šè¯·å‹¿åœ¨æ—¥å¿—æˆ–æ§åˆ¶å°ä¸­æš´éœ²GitHub tokenç­‰æ•æ„Ÿä¿¡æ¯")
            return False

        # ä¸Šä¼ æ–‡ä»¶
        results = {}

        print("å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°GitHub...")

        files_to_upload = github_config.get('files_to_upload', [])
        for file_config in files_to_upload:
            local_path = file_config['local_path']
            github_path = file_config['github_path']

            print(f"å¤„ç†æ–‡ä»¶: {local_path} -> {github_path}")
            success = self.upload_file_to_github(local_path, github_path)
            results[local_path] = success

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)

        print(f"ä¸Šä¼ å®Œæˆ: {success_count}/{total_count} ä¸ªæ–‡ä»¶æˆåŠŸ")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„ä¸Šä¼ 
        failed_files = [path for path, success in results.items() if not success]
        if failed_files:
            print(f"ä»¥ä¸‹æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {failed_files}")
            return False

        print("æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
        return True


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='Pan Site Monitor - TVBoxèµ„æºç«™ç‚¹ç›‘æ§å·¥å…·')
    parser.add_argument('command', choices=['tvbox', 'test', 'upload', 'all', 'quick'],
                       help='æ‰§è¡Œçš„å‘½ä»¤: tvbox(TVBoxç®¡ç†), test(URLæµ‹è¯•), upload(GitHubä¸Šä¼ ), all(å…¨éƒ¨), quick(å¿«é€Ÿæ¨¡å¼ï¼šä»…æµ‹é€Ÿ+ä¸Šä¼ )')
    parser.add_argument('--config', default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-update', action='store_true', help='è·³è¿‡TVBoxç‰ˆæœ¬æ£€æŸ¥')
    parser.add_argument('--no-aggregate', action='store_true', help='è·³è¿‡æ•°æ®èšåˆ')

    args = parser.parse_args()

    try:
        monitor = PanSiteMonitor(args.config)

        if args.command == 'tvbox':
            print("=== TVBoxèµ„æºç®¡ç† ===")
            results = monitor.run_tvbox_manager(
                check_update=not args.no_update,
                aggregate_data=not args.no_aggregate
            )
            success = results['update'] and results['aggregate']

        elif args.command == 'test':
            print("=== URLå¯ç”¨æ€§æµ‹è¯• ===")
            results = monitor.run_url_tester()
            success = len(results) > 0

        elif args.command == 'upload':
            print("=== GitHubæ–‡ä»¶ä¸Šä¼  ===")
            success = monitor.run_github_uploader()

        elif args.command == 'quick':
            print("=== å¿«é€Ÿæ¨¡å¼ï¼šæµ‹é€Ÿ+ä¸Šä¼  ===")
            print("â„¹ï¸  è·³è¿‡TVBoxç®¡ç†ï¼Œç›´æ¥ä½¿ç”¨data/test.jsonè¿›è¡Œæµ‹é€Ÿå’Œä¸Šä¼ ")

            # 1. URLæµ‹è¯•
            print("\n1. URLå¯ç”¨æ€§æµ‹è¯•")
            test_results = monitor.run_url_tester()

            if not test_results:
                print("URLæµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡GitHubä¸Šä¼ ")
                sys.exit(1)

            # 2. GitHubä¸Šä¼ 
            print("\n2. GitHubæ–‡ä»¶ä¸Šä¼ ")
            upload_success = monitor.run_github_uploader()

            success = upload_success

        elif args.command == 'all':
            print("=== æ‰§è¡Œå®Œæ•´æµç¨‹ ===")

            # 1. TVBoxç®¡ç†
            print("\n1. TVBoxèµ„æºç®¡ç†")
            tvbox_results = monitor.run_tvbox_manager(
                check_update=not args.no_update,
                aggregate_data=not args.no_aggregate
            )

            if not (tvbox_results['update'] and tvbox_results['aggregate']):
                print("TVBoxç®¡ç†å¤±è´¥ï¼Œè·³è¿‡åç»­æ­¥éª¤")
                sys.exit(1)

            # 2. URLæµ‹è¯•
            print("\n2. URLå¯ç”¨æ€§æµ‹è¯•")
            test_results = monitor.run_url_tester()

            if not test_results:
                print("URLæµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡GitHubä¸Šä¼ ")
                sys.exit(1)

            # 3. GitHubä¸Šä¼ 
            print("\n3. GitHubæ–‡ä»¶ä¸Šä¼ ")
            upload_success = monitor.run_github_uploader()

            success = upload_success

        else:
            print(f"æœªçŸ¥å‘½ä»¤: {args.command}")
            sys.exit(1)

        if success:
            print(f"\nâœ… {args.command} å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            print(f"\nâŒ {args.command} å‘½ä»¤æ‰§è¡Œå¤±è´¥")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥å°†è¯¦ç»†é”™è¯¯ä¿¡æ¯è®°å½•åˆ°æ—¥å¿—è€Œä¸æ˜¯æ‰“å°åˆ°æ§åˆ¶å°
        # é¿å…åœ¨æ§åˆ¶å°è¾“å‡ºå¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯çš„å®Œæ•´å †æ ˆè·Ÿè¸ª
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            os.makedirs("logs", exist_ok=True)
            
            # å°è¯•åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„loggeræ¥è®°å½•è¯¦ç»†é”™è¯¯
            import logging
            error_logger = logging.getLogger("pan_monitor_error")
            if not error_logger.handlers:
                handler = logging.FileHandler("logs/error.log", encoding='utf-8')
                formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                error_logger.addHandler(handler)
                error_logger.setLevel(logging.ERROR)

            import traceback
            error_logger.error(f"ç¨‹åºå¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯å·²è®°å½•åˆ° logs/error.log")
        except Exception as log_error:
            # å¦‚æœæ—¥å¿—è®°å½•ä¹Ÿå¤±è´¥ï¼Œåˆ™è®°å½•åŸå§‹å¼‚å¸¸å’Œæ—¥å¿—å¼‚å¸¸
            print(f"æ— æ³•è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯: {log_error}")
            print(f"åŸå§‹å¼‚å¸¸: {e}")

        sys.exit(1)


if __name__ == "__main__":
    main()
